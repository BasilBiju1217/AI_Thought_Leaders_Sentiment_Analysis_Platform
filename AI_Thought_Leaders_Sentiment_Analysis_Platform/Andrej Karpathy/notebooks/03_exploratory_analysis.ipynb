{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c4db009-1809-43d1-8238-82c9a3260708",
   "metadata": {},
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3c0e54-eb37-4b96-b178-a828f2ef8e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q pandas pyarrow numpy seaborn matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001e2172-7d40-4de4-94b9-c7562c8fba58",
   "metadata": {},
   "source": [
    "# 1) Setup: paths and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352f2ca8-66e6-41a4-8648-9c2f5ca94f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and paths\n",
    "import os, re, json, math\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "# Paths (works whether you run from notebooks/ or project root)\n",
    "PROJ = Path.cwd().resolve().parents[0] if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "DATA = PROJ / \"data\"\n",
    "RAW = DATA / \"raw\"\n",
    "PROC = DATA / \"processed\"\n",
    "REPORTS = PROJ / \"reports\"\n",
    "FIGS = REPORTS / \"figures\"\n",
    "for p in [RAW, PROC, REPORTS, FIGS]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def timestamp():\n",
    "    return datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def latest_file(folder: Path, pattern=\"*.parquet\"):\n",
    "    files = sorted(folder.glob(pattern), key=lambda p: p.stat().st_mtime)\n",
    "    return files[-1] if files else None\n",
    "\n",
    "RUN_ID = timestamp()\n",
    "RUN_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c904b52e-cf97-4a0d-b16e-5c6e4fdfad74",
   "metadata": {},
   "source": [
    "# 2) Load latest processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0aba8b-46b1-44e5-bd4b-c7ad70899e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_path = latest_file(PROC, \"*.parquet\")\n",
    "if not proc_path:\n",
    "    raise FileNotFoundError(\"No processed parquet found in data/processed. Run 02_data_cleaning first.\")\n",
    "\n",
    "print(f\"Loading processed snapshot: {proc_path}\")\n",
    "df = pd.read_parquet(proc_path)\n",
    "\n",
    "# Ensure expected columns exist\n",
    "if \"text_clean\" not in df.columns:\n",
    "    raise ValueError(\"Expected 'text_clean' column not found. Re-run 02_data_cleaning.\")\n",
    "\n",
    "# Types and convenience columns\n",
    "if \"created_at\" in df.columns:\n",
    "    df[\"created_at\"] = pd.to_datetime(df[\"created_at\"], errors=\"coerce\")\n",
    "\n",
    "# Reconstruct 'type' if missing\n",
    "if \"type\" not in df.columns:\n",
    "    if \"is_retweet\" in df.columns and \"is_quote\" in df.columns:\n",
    "        df[\"type\"] = np.where(df[\"is_retweet\"], \"retweet\",\n",
    "                              np.where(df[\"is_quote\"], \"quote\", \"original\"))\n",
    "    else:\n",
    "        df[\"type\"] = \"unknown\"\n",
    "\n",
    "# Ensure char_len/word_count exist\n",
    "if \"char_len\" not in df.columns:\n",
    "    df[\"char_len\"] = df[\"text_clean\"].astype(str).str.len()\n",
    "if \"word_count\" not in df.columns:\n",
    "    df[\"word_count\"] = df[\"text_clean\"].astype(str).str.split().apply(len)\n",
    "\n",
    "rows = len(df)\n",
    "cols = len(df.columns)\n",
    "rows, cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4804da-49b4-4a88-ab10-c04deed0750e",
   "metadata": {},
   "source": [
    "# 3) Dataset overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7653e4c3-06dd-4104-a6dd-9444af04a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "print(\"Head:\")\n",
    "display(df.head(3))\n",
    "\n",
    "print(\"\\nShape:\", df.shape)\n",
    "print(\"\\nColumns:\", df.columns.tolist())\n",
    "\n",
    "print(\"\\nDtypes:\")\n",
    "display(df.dtypes)\n",
    "\n",
    "print(\"\\nLanguage distribution (top 10):\")\n",
    "if \"lang\" in df.columns:\n",
    "    display(df[\"lang\"].value_counts().head(10))\n",
    "\n",
    "print(\"\\nType distribution:\")\n",
    "display(df[\"type\"].value_counts())\n",
    "\n",
    "if \"username\" in df.columns:\n",
    "    print(\"\\nTop usernames:\")\n",
    "    display(df[\"username\"].value_counts().head(10))\n",
    "\n",
    "if {\"like_count\", \"retweet_count\", \"comment_count\"}.issubset(df.columns):\n",
    "    print(\"\\nEngagement summary:\")\n",
    "    display(df[[\"like_count\",\"retweet_count\",\"comment_count\"]].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e5c692-f6c3-4afa-a3d5-bb2bd956e111",
   "metadata": {},
   "source": [
    "# 4) Time trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779da552-78d0-4321-955f-3bc8792d8b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_time = \"created_at\" in df.columns and df[\"created_at\"].notna().any()\n",
    "if has_time:\n",
    "    # Daily volume\n",
    "    daily = (df.set_index(\"created_at\")\n",
    "               .assign(count=1)\n",
    "               .resample(\"D\")[\"count\"].sum()\n",
    "               .reset_index())\n",
    "\n",
    "    plt.figure(figsize=(8,3))\n",
    "    sns.lineplot(data=daily, x=\"created_at\", y=\"count\", marker=\"o\", linewidth=1)\n",
    "    plt.title(\"Posts per day\")\n",
    "    plt.xlabel(\"date\")\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.tight_layout()\n",
    "    fig_path = FIGS / f\"eda_posts_per_day_{RUN_ID}.png\"\n",
    "    plt.savefig(fig_path)\n",
    "    plt.show()\n",
    "    print(f\"Saved: {fig_path}\")\n",
    "\n",
    "    # Hour-of-day and weekday patterns\n",
    "    tmp = df[df[\"created_at\"].notna()].copy()\n",
    "    tmp[\"hour\"] = tmp[\"created_at\"].dt.hour\n",
    "    tmp[\"weekday\"] = tmp[\"created_at\"].dt.dayofweek  # 0=Mon\n",
    "    tmp[\"weekday_name\"] = tmp[\"created_at\"].dt.day_name()\n",
    "\n",
    "    plt.figure(figsize=(7,3))\n",
    "    sns.countplot(data=tmp, x=\"hour\", color=\"#4C78A8\")\n",
    "    plt.title(\"Posts by hour of day\")\n",
    "    plt.tight_layout()\n",
    "    fig_path = FIGS / f\"eda_posts_by_hour_{RUN_ID}.png\"\n",
    "    plt.savefig(fig_path)\n",
    "    plt.show()\n",
    "    print(f\"Saved: {fig_path}\")\n",
    "\n",
    "    plt.figure(figsize=(7,3))\n",
    "    order = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    "    sns.countplot(data=tmp, x=\"weekday_name\", order=order, color=\"#72B7B2\")\n",
    "    plt.title(\"Posts by weekday\")\n",
    "    plt.xticks(rotation=30, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    fig_path = FIGS / f\"eda_posts_by_weekday_{RUN_ID}.png\"\n",
    "    plt.savefig(fig_path)\n",
    "    plt.show()\n",
    "    print(f\"Saved: {fig_path}\")\n",
    "\n",
    "    # Heatmap (weekday x hour)\n",
    "    pivot = (tmp.groupby([\"weekday_name\",\"hour\"]).size()\n",
    "                .reset_index(name=\"count\")\n",
    "                .pivot(index=\"weekday_name\", columns=\"hour\", values=\"count\")\n",
    "                .reindex(index=order))\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.heatmap(pivot.fillna(0), cmap=\"Blues\")\n",
    "    plt.title(\"Post volume: weekday x hour\")\n",
    "    plt.tight_layout()\n",
    "    fig_path = FIGS / f\"eda_heatmap_weekday_hour_{RUN_ID}.png\"\n",
    "    plt.savefig(fig_path)\n",
    "    plt.show()\n",
    "    print(f\"Saved: {fig_path}\")\n",
    "\n",
    "    # Engagement over time (if present)\n",
    "    if {\"like_count\",\"retweet_count\"}.issubset(df.columns):\n",
    "        daily_eng = (df[df[\"created_at\"].notna()]\n",
    "                       .set_index(\"created_at\")\n",
    "                       .resample(\"D\")[[\"like_count\",\"retweet_count\",\"comment_count\"]]\n",
    "                       .mean())\n",
    "        plt.figure(figsize=(8,3))\n",
    "        sns.lineplot(data=daily_eng)\n",
    "        plt.title(\"Average engagement per day\")\n",
    "        plt.xlabel(\"date\")\n",
    "        plt.tight_layout()\n",
    "        fig_path = FIGS / f\"eda_engagement_per_day_{RUN_ID}.png\"\n",
    "        plt.savefig(fig_path)\n",
    "        plt.show()\n",
    "        print(f\"Saved: {fig_path}\")\n",
    "else:\n",
    "    print(\"created_at not available; skipping time series.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07945d67-b15f-4b42-8028-4ae84541a064",
   "metadata": {},
   "source": [
    "# 5) Length distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcd6505-e9ac-4a1d-bf1e-3a1d7e8e7ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,3))\n",
    "sns.histplot(df[\"char_len\"], bins=50, color=\"#F58518\")\n",
    "plt.title(\"Cleaned text length (characters)\")\n",
    "plt.tight_layout()\n",
    "fig_path = FIGS / f\"eda_char_len_{RUN_ID}.png\"\n",
    "plt.savefig(fig_path)\n",
    "plt.show()\n",
    "print(f\"Saved: {fig_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbfea6b-5d29-452f-a497-42be6b3fcbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,3))\n",
    "sns.histplot(df[\"word_count\"], bins=50, color=\"#54A24B\")\n",
    "plt.title(\"Cleaned text length (words)\")\n",
    "plt.tight_layout()\n",
    "fig_path = FIGS / f\"eda_word_count_{RUN_ID}.png\"\n",
    "plt.savefig(fig_path)\n",
    "plt.show()\n",
    "print(f\"Saved: {fig_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c4c58e-37f8-4729-a65b-089a3aa11275",
   "metadata": {},
   "source": [
    "# 6) Engagement distributions and top posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c76695-7e90-43a8-8eed-ff94b1a81f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "if {\"like_count\",\"retweet_count\",\"comment_count\"}.issubset(df.columns):\n",
    "    # Distributions (clip to reduce long tails in plots)\n",
    "    for col, color in [(\"like_count\",\"#4C78A8\"), (\"retweet_count\",\"#72B7B2\"), (\"comment_count\",\"#E45756\")]:\n",
    "        plt.figure(figsize=(7,3))\n",
    "        sns.histplot(df[col].clip(upper=np.quantile(df[col], 0.99)), bins=50, color=color)\n",
    "        plt.title(f\"{col} distribution (clipped 99th pct)\")\n",
    "        plt.tight_layout()\n",
    "        fig_path = FIGS / f\"eda_{col}_dist_{RUN_ID}.png\"\n",
    "        plt.savefig(fig_path)\n",
    "        plt.show()\n",
    "        print(f\"Saved: {fig_path}\")\n",
    "\n",
    "    # Top posts by likes and retweets\n",
    "    top_by_likes = df.sort_values(\"like_count\", ascending=False).head(20)\n",
    "    top_by_retweets = df.sort_values(\"retweet_count\", ascending=False).head(20)\n",
    "\n",
    "    # Save CSVs (serialize URLs if list)\n",
    "    def _serialize_urls(s):\n",
    "        if isinstance(s, (list, tuple)): return json.dumps(s, ensure_ascii=False)\n",
    "        return s\n",
    "    out1 = REPORTS / f\"top_posts_by_likes_{RUN_ID}.csv\"\n",
    "    out2 = REPORTS / f\"top_posts_by_retweets_{RUN_ID}.csv\"\n",
    "    tmp1, tmp2 = top_by_likes.copy(), top_by_retweets.copy()\n",
    "    if \"urls\" in tmp1.columns: tmp1[\"urls\"] = tmp1[\"urls\"].map(_serialize_urls)\n",
    "    if \"urls\" in tmp2.columns: tmp2[\"urls\"] = tmp2[\"urls\"].map(_serialize_urls)\n",
    "    tmp1.to_csv(out1, index=False, encoding=\"utf-8\")\n",
    "    tmp2.to_csv(out2, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Saved:\\n- {out1}\\n- {out2}\")\n",
    "else:\n",
    "    print(\"Engagement columns not available; skipping engagement plots and top posts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2ffcbb-650a-48f3-9cf0-1c90f7589ec5",
   "metadata": {},
   "source": [
    "# 7) Top tokens and n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03535bc0-9eb7-4b56-9dc5-dad1a3965a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_ngrams(texts, topk=30, ngram_range=(1,2), min_df=2, max_features=40000, stop_words=\"english\"):\n",
    "    texts = pd.Series(texts).fillna(\"\").astype(str).tolist()\n",
    "    if not any(len(t.strip()) for t in texts):\n",
    "        return pd.DataFrame(columns=[\"term\",\"freq\"])\n",
    "    vec = CountVectorizer(stop_words=stop_words, ngram_range=ngram_range,\n",
    "                          min_df=min_df, max_features=max_features)\n",
    "    X = vec.fit_transform(texts)\n",
    "    vocab = np.array(vec.get_feature_names_out())\n",
    "    freqs = np.asarray(X.sum(axis=0)).ravel()\n",
    "    order = np.argsort(freqs)[::-1][:topk]\n",
    "    return pd.DataFrame({\"term\": vocab[order], \"freq\": freqs[order]})\n",
    "\n",
    "# Overall top terms\n",
    "top_terms = top_ngrams(df[\"text_clean\"], topk=30, ngram_range=(1,2), min_df=2)\n",
    "display(top_terms)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(data=top_terms, y=\"term\", x=\"freq\", color=\"#4C78A8\")\n",
    "plt.title(\"Top terms (unigrams + bigrams)\")\n",
    "plt.xlabel(\"frequency\")\n",
    "plt.ylabel(\"\")\n",
    "plt.tight_layout()\n",
    "fig_path = FIGS / f\"eda_top_terms_{RUN_ID}.png\"\n",
    "plt.savefig(fig_path)\n",
    "plt.show()\n",
    "print(f\"Saved: {fig_path}\")\n",
    "\n",
    "# Top terms by type (if multiple types present)\n",
    "if df[\"type\"].nunique() > 1:\n",
    "    for t in df[\"type\"].value_counts().index.tolist():\n",
    "        sub = df[df[\"type\"] == t]\n",
    "        if len(sub) < 10:\n",
    "            continue\n",
    "        tt = top_ngrams(sub[\"text_clean\"], topk=20, ngram_range=(1,2), min_df=2)\n",
    "        plt.figure(figsize=(7,5))\n",
    "        sns.barplot(data=tt, y=\"term\", x=\"freq\", color=\"#72B7B2\")\n",
    "        plt.title(f\"Top terms: {t}\")\n",
    "        plt.xlabel(\"frequency\")\n",
    "        plt.ylabel(\"\")\n",
    "        plt.tight_layout()\n",
    "        fig_path = FIGS / f\"eda_top_terms_{t}_{RUN_ID}.png\"\n",
    "        plt.savefig(fig_path)\n",
    "        plt.show()\n",
    "        print(f\"Saved: {fig_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de0f3fe-1831-4752-ae4c-c9f1c6001c17",
   "metadata": {},
   "source": [
    "# 8) Hashtags, mentions, and URLs/domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac94612-62c6-4989-a23c-a14bcfcf4c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract from original text (not cleaned), if available\n",
    "HAS_TEXT = \"text\" in df.columns\n",
    "\n",
    "# Hashtags\n",
    "if HAS_TEXT:\n",
    "    hashtag_re = re.compile(r\"#(\\w+)\")\n",
    "    all_tags = df[\"text\"].dropna().astype(str).map(lambda s: hashtag_re.findall(s))\n",
    "    tags = [t.lower() for lst in all_tags for t in lst]\n",
    "    tags_s = pd.Series(tags)\n",
    "    if not tags_s.empty:\n",
    "        top_hashtags = tags_s.value_counts().head(30).reset_index()\n",
    "        top_hashtags.columns = [\"hashtag\",\"freq\"]\n",
    "        display(top_hashtags)\n",
    "\n",
    "        plt.figure(figsize=(8,6))\n",
    "        sns.barplot(data=top_hashtags, y=\"hashtag\", x=\"freq\", color=\"#E45756\")\n",
    "        plt.title(\"Top hashtags\")\n",
    "        plt.tight_layout()\n",
    "        fig_path = FIGS / f\"eda_top_hashtags_{RUN_ID}.png\"\n",
    "        plt.savefig(fig_path)\n",
    "        plt.show()\n",
    "        print(f\"Saved: {fig_path}\")\n",
    "    else:\n",
    "        print(\"No hashtags found.\")\n",
    "else:\n",
    "    print(\"Original 'text' column not available; skipping hashtag analysis.\")\n",
    "\n",
    "# Mentions\n",
    "if HAS_TEXT:\n",
    "    mention_re = re.compile(r\"@([A-Za-z0-9_]{1,15})\")\n",
    "    all_mentions = df[\"text\"].dropna().astype(str).map(lambda s: mention_re.findall(s))\n",
    "    mentions = [m.lower() for lst in all_mentions for m in lst]\n",
    "    mentions_s = pd.Series(mentions)\n",
    "    if not mentions_s.empty:\n",
    "        top_mentions = mentions_s.value_counts().head(30).reset_index()\n",
    "        top_mentions.columns = [\"mention\",\"freq\"]\n",
    "        display(top_mentions)\n",
    "\n",
    "        plt.figure(figsize=(8,6))\n",
    "        sns.barplot(data=top_mentions, y=\"mention\", x=\"freq\", color=\"#F58518\")\n",
    "        plt.title(\"Top mentions\")\n",
    "        plt.tight_layout()\n",
    "        fig_path = FIGS / f\"eda_top_mentions_{RUN_ID}.png\"\n",
    "        plt.savefig(fig_path)\n",
    "        plt.show()\n",
    "        print(f\"Saved: {fig_path}\")\n",
    "    else:\n",
    "        print(\"No mentions found.\")\n",
    "\n",
    "# URLs and domains (from 'urls' list)\n",
    "if \"urls\" in df.columns:\n",
    "    def extract_domains(urls):\n",
    "        if not isinstance(urls, (list, tuple)): return []\n",
    "        ds = []\n",
    "        for u in urls:\n",
    "            try:\n",
    "                d = urlparse(u).netloc.lower()\n",
    "                if d.startswith(\"www.\"): d = d[4:]\n",
    "                ds.append(d)\n",
    "            except:\n",
    "                continue\n",
    "        return ds\n",
    "\n",
    "    domains = []\n",
    "    for lst in df[\"urls\"].fillna([]):\n",
    "        domains.extend(extract_domains(lst))\n",
    "\n",
    "    dom_s = pd.Series(domains)\n",
    "    if not dom_s.empty:\n",
    "        top_domains = dom_s.value_counts().head(30).reset_index()\n",
    "        top_domains.columns = [\"domain\",\"freq\"]\n",
    "        display(top_domains)\n",
    "\n",
    "        plt.figure(figsize=(8,6))\n",
    "        sns.barplot(data=top_domains, y=\"domain\", x=\"freq\", color=\"#54A24B\")\n",
    "        plt.title(\"Top linked domains\")\n",
    "        plt.tight_layout()\n",
    "        fig_path = FIGS / f\"eda_top_domains_{RUN_ID}.png\"\n",
    "        plt.savefig(fig_path)\n",
    "        plt.show()\n",
    "        print(f\"Saved: {fig_path}\")\n",
    "\n",
    "        # Save CSV\n",
    "        out_domains = REPORTS / f\"top_domains_{RUN_ID}.csv\"\n",
    "        top_domains.to_csv(out_domains, index=False, encoding=\"utf-8\")\n",
    "        print(f\"Saved: {out_domains}\")\n",
    "    else:\n",
    "        print(\"No external URLs found.\")\n",
    "else:\n",
    "    print(\"'urls' column not available; skipping URL/domain analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553c608a-79d0-4754-8501-accf62dbf287",
   "metadata": {},
   "source": [
    "# 9) Save EDA summary artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680862ed-0417-447c-8422-c1c8e7a4b456",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"input\": str(proc_path.relative_to(PROJ)),\n",
    "    \"rows\": int(len(df)),\n",
    "    \"columns\": df.columns.tolist(),\n",
    "    \"time_range\": {\n",
    "        \"min\": (df[\"created_at\"].min().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                if \"created_at\" in df.columns and df[\"created_at\"].notna().any() else None),\n",
    "        \"max\": (df[\"created_at\"].max().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                if \"created_at\" in df.columns and df[\"created_at\"].notna().any() else None),\n",
    "    },\n",
    "    \"lang_counts_top10\": (df[\"lang\"].value_counts().head(10).to_dict() if \"lang\" in df.columns else {}),\n",
    "    \"type_counts\": df[\"type\"].value_counts().to_dict(),\n",
    "    \"username_top10\": (df[\"username\"].value_counts().head(10).to_dict() if \"username\" in df.columns else {}),\n",
    "    \"length_stats\": df[[\"char_len\",\"word_count\"]].describe().to_dict(),\n",
    "    \"engagement_stats\": (df[[\"like_count\",\"retweet_count\",\"comment_count\"]].describe().to_dict()\n",
    "                         if {\"like_count\",\"retweet_count\",\"comment_count\"}.issubset(df.columns) else {}),\n",
    "}\n",
    "\n",
    "summary_path = REPORTS / f\"eda_summary_{RUN_ID}.json\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"Saved EDA summary: {summary_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
