{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "382b7daf-8a04-4fd8-8fcb-2793697e2737",
   "metadata": {},
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e169fbec-b59f-491e-bf13-cc81a33f9aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q pandas pyarrow ftfy seaborn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22520912-fd46-4705-952b-8455259ccba9",
   "metadata": {},
   "source": [
    "# 1) Paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c6dcf9-4f38-4647-8c23-20ea56e0b842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and paths\n",
    "import os, re, json, unicodedata\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Paths (works whether you run from notebooks/ or project root)\n",
    "PROJ = Path.cwd().resolve().parents[0] if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "DATA = PROJ / \"data\"\n",
    "RAW = DATA / \"raw\"\n",
    "PROC = DATA / \"processed\"\n",
    "MODELS = PROJ / \"models\"\n",
    "REPORTS = PROJ / \"reports\"\n",
    "FIGS = REPORTS / \"figures\"\n",
    "for p in [RAW, PROC, MODELS, REPORTS, FIGS]: p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def timestamp():\n",
    "    return datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def latest_file(folder: Path, pattern=\"*.parquet\"):\n",
    "    files = sorted(folder.glob(pattern), key=lambda p: p.stat().st_mtime)\n",
    "    return files[-1] if files else None\n",
    "\n",
    "RUN_ID = timestamp()\n",
    "RUN_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3a4ee3-b09f-464c-af32-33aa25bba6d5",
   "metadata": {},
   "source": [
    "# 2) Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb146e69-d879-4d6f-8a24-6db29c7f86ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language filter: keep only these languages if not None\n",
    "LANGS_KEEP = [\"en\"]  # set to None to keep all languages\n",
    "\n",
    "# Content filters\n",
    "KEEP_RETWEETS = False\n",
    "KEEP_QUOTES = True\n",
    "KEEP_REPLIES = True\n",
    "\n",
    "# Minimum cleaned text length (characters)\n",
    "MIN_CHAR_LEN = 3\n",
    "\n",
    "# Heuristic to drop URL-only or almost-URL tweets\n",
    "DROP_URL_HEAVY = True\n",
    "URL_HEAVY_THRESHOLD = 0.7  # fraction of characters that are part of URLs\n",
    "\n",
    "# Optional date filtering (UTC naive)\n",
    "DATE_FROM = None  # e.g., \"2023-01-01\"\n",
    "DATE_TO = None    # e.g., \"2024-12-31\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddc8bdf-ec86-410e-9cf9-599eaf096a4a",
   "metadata": {},
   "source": [
    "# 3) Load latest raw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb1f4ec-9198-4f93-8fd9-00dcae33d533",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = latest_file(RAW, \"*.parquet\")\n",
    "if not raw_path:\n",
    "    raise FileNotFoundError(\"No raw parquet found in data/raw. Run 01_data_collection first.\")\n",
    "print(f\"Loading raw snapshot: {raw_path}\")\n",
    "\n",
    "df = pd.read_parquet(raw_path)\n",
    "\n",
    "# Basic validations\n",
    "if \"text\" not in df.columns:\n",
    "    raise ValueError(\"Expected 'text' column not found in raw data.\")\n",
    "if \"created_at\" in df.columns:\n",
    "    df[\"created_at\"] = pd.to_datetime(df[\"created_at\"], errors=\"coerce\")\n",
    "rows_initial = len(df)\n",
    "rows_initial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d21561a-a9cc-4489-9cc5-7849a58aff0b",
   "metadata": {},
   "source": [
    "# 4) Cleaning helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473a0132-e952-4e2b-aeee-e4b4ed4f7b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftfy\n",
    "\n",
    "URL_RE = re.compile(r\"(https?://\\S+|www\\.\\S+)\", flags=re.IGNORECASE)\n",
    "MENTION_RE = re.compile(r\"@\\w+\")\n",
    "WHITESPACE_RE = re.compile(r\"\\s+\")\n",
    "\n",
    "def strip_urls(s: str) -> str:\n",
    "    return URL_RE.sub(\"\", s)\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = ftfy.fix_text(s)\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    # Remove URLs, keep words around them\n",
    "    s = strip_urls(s)\n",
    "    # Remove mentions\n",
    "    s = MENTION_RE.sub(\"\", s)\n",
    "    # Remove hash sign but keep the word (e.g., #AI -> AI)\n",
    "    s = s.replace(\"#\", \"\")\n",
    "    # Collapse whitespace\n",
    "    s = WHITESPACE_RE.sub(\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def url_char_fraction(original: str) -> float:\n",
    "    if not isinstance(original, str) or not original:\n",
    "        return 0.0\n",
    "    matches = URL_RE.findall(original)\n",
    "    total_url_chars = sum(len(m) for m in matches)\n",
    "    return total_url_chars / max(1, len(original))\n",
    "\n",
    "def stage_diff(before: int, after: int, name: str, tracker: Dict[str, Any]):\n",
    "    tracker[name] = {\"removed\": before - after, \"before\": before, \"after\": after}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a6e36b-3b57-4ec9-91cb-45ea081bb190",
   "metadata": {},
   "source": [
    "# 5) Apply cleaning and filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc1a307-757f-48b7-94e9-6943de09594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "removals = {}\n",
    "\n",
    "# Standardize columns that might be missing\n",
    "for col, default in [\n",
    "    (\"is_retweet\", False),\n",
    "    (\"is_quote\", False),\n",
    "    (\"is_reply\", False),\n",
    "]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = default\n",
    "\n",
    "# Drop rows with missing/empty text\n",
    "b = len(df)\n",
    "df = df[df[\"text\"].notna()]\n",
    "df[\"text\"] = df[\"text\"].astype(str).str.strip()\n",
    "df = df[df[\"text\"].str.len() > 0]\n",
    "stage_diff(b, len(df), \"drop_empty_text\", removals)\n",
    "\n",
    "# Language filter (use existing 'lang' column if present)\n",
    "if LANGS_KEEP is not None:\n",
    "    if \"lang\" in df.columns:\n",
    "        b = len(df)\n",
    "        df = df[df[\"lang\"].isin(LANGS_KEEP)]\n",
    "        stage_diff(b, len(df), \"filter_language\", removals)\n",
    "    else:\n",
    "        print(\"LANGS_KEEP set but 'lang' column not found. Skipping language filter.\")\n",
    "\n",
    "# Drop retweets/quotes/replies per params\n",
    "if not KEEP_RETWEETS and \"is_retweet\" in df.columns:\n",
    "    b = len(df)\n",
    "    df = df[~df[\"is_retweet\"]]\n",
    "    stage_diff(b, len(df), \"filter_retweets\", removals)\n",
    "\n",
    "if not KEEP_QUOTES and \"is_quote\" in df.columns:\n",
    "    b = len(df)\n",
    "    df = df[~df[\"is_quote\"]]\n",
    "    stage_diff(b, len(df), \"filter_quotes\", removals)\n",
    "\n",
    "if not KEEP_REPLIES and \"is_reply\" in df.columns:\n",
    "    b = len(df)\n",
    "    df = df[~df[\"is_reply\"]]\n",
    "    stage_diff(b, len(df), \"filter_replies\", removals)\n",
    "\n",
    "# Date filters if provided\n",
    "if DATE_FROM and \"created_at\" in df.columns:\n",
    "    b = len(df)\n",
    "    df = df[df[\"created_at\"] >= pd.to_datetime(DATE_FROM)]\n",
    "    stage_diff(b, len(df), \"filter_date_from\", removals)\n",
    "\n",
    "if DATE_TO and \"created_at\" in df.columns:\n",
    "    b = len(df)\n",
    "    df = df[df[\"created_at\"] <= pd.to_datetime(DATE_TO)]\n",
    "    stage_diff(b, len(df), \"filter_date_to\", removals)\n",
    "\n",
    "# Clean text\n",
    "df[\"text_clean\"] = df[\"text\"].map(clean_text)\n",
    "\n",
    "# Heuristic removal for URL-heavy posts\n",
    "if DROP_URL_HEAVY:\n",
    "    b = len(df)\n",
    "    frac = df[\"text\"].map(url_char_fraction)\n",
    "    df = df[frac < URL_HEAVY_THRESHOLD]\n",
    "    stage_diff(b, len(df), \"filter_url_heavy\", removals)\n",
    "\n",
    "# Length-based filtering on cleaned text\n",
    "df[\"char_len\"] = df[\"text_clean\"].str.len()\n",
    "b = len(df)\n",
    "df = df[df[\"char_len\"] >= MIN_CHAR_LEN]\n",
    "stage_diff(b, len(df), \"filter_min_char_len\", removals)\n",
    "\n",
    "# Word count feature (optional, useful later)\n",
    "df[\"word_count\"] = df[\"text_clean\"].str.split().apply(len)\n",
    "\n",
    "# Deduplicate\n",
    "b = len(df)\n",
    "if \"tweet_id\" in df.columns:\n",
    "    df = df.drop_duplicates(subset=[\"tweet_id\"])\n",
    "stage_diff(b, len(df), \"dedupe_tweet_id\", removals)\n",
    "\n",
    "# Secondary dedupe on username + normalized text (case-insensitive)\n",
    "df[\"text_norm\"] = df[\"text_clean\"].str.lower()\n",
    "b = len(df)\n",
    "subset_cols = [\"username\", \"text_norm\"] if \"username\" in df.columns else [\"text_norm\"]\n",
    "df = df.drop_duplicates(subset=subset_cols)\n",
    "stage_diff(b, len(df), \"dedupe_username_text\", removals)\n",
    "\n",
    "# Sort by time if available\n",
    "if \"created_at\" in df.columns:\n",
    "    df = df.sort_values(\"created_at\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "rows_final = len(df)\n",
    "print(f\"Rows: start={rows_initial:,} -> final={rows_final:,}\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0975b1-45c5-476d-9386-48a344478f3f",
   "metadata": {},
   "source": [
    "# 6) Quick QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d95ec7e-7728-4ff7-b814-df305eaabaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Language distribution (top 10):\")\n",
    "if \"lang\" in df.columns:\n",
    "    display(df[\"lang\"].value_counts().head(10))\n",
    "\n",
    "print(\"\\nType distribution:\")\n",
    "if \"is_retweet\" in df.columns and \"is_quote\" in df.columns:\n",
    "    df[\"type\"] = np.where(df[\"is_retweet\"], \"retweet\", np.where(df[\"is_quote\"], \"quote\", \"original\"))\n",
    "    display(df[\"type\"].value_counts())\n",
    "\n",
    "print(\"\\nLength stats:\")\n",
    "display(df[[\"char_len\", \"word_count\"]].describe())\n",
    "\n",
    "# Plot length distribution\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(df[\"char_len\"], bins=50, color=\"#4C78A8\")\n",
    "plt.title(\"Cleaned text length distribution\")\n",
    "plt.xlabel(\"characters\")\n",
    "plt.tight_layout()\n",
    "fig_path = FIGS / f\"length_distribution_clean_{RUN_ID}.png\"\n",
    "plt.savefig(fig_path, dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved figure: {fig_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0067f23d-ab5a-4cb8-9f02-d769023e42c6",
   "metadata": {},
   "source": [
    "# 7) Save processed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0711c07a-9f8a-47c5-94c1-2cfc50117f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = RUN_ID\n",
    "proc_path = PROC / f\"x_posts_clean_{ts}.parquet\"\n",
    "csv_sample_path = PROC / f\"x_posts_clean_sample_{ts}.csv\"\n",
    "manifest_path = PROC / f\"manifest_clean_{ts}.json\"\n",
    "\n",
    "# Save Parquet (lists like 'urls' will be preserved by pyarrow)\n",
    "df.to_parquet(proc_path, index=False)\n",
    "\n",
    "# Also save a small CSV sample for quick inspection (first 1,000 rows)\n",
    "df_csv = df.copy()\n",
    "if \"urls\" in df_csv.columns:\n",
    "    df_csv[\"urls\"] = df_csv[\"urls\"].apply(lambda v: json.dumps(v) if isinstance(v, (list, tuple)) else v)\n",
    "df_csv.head(1000).to_csv(csv_sample_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "manifest = {\n",
    "    \"run_id\": ts,\n",
    "    \"raw_input\": str(raw_path.relative_to(PROJ)),\n",
    "    \"processed_output\": str(proc_path.relative_to(PROJ)),\n",
    "    \"csv_sample\": str(csv_sample_path.relative_to(PROJ)),\n",
    "    \"rows_initial\": int(rows_initial),\n",
    "    \"rows_final\": int(rows_final),\n",
    "    \"columns\": df.columns.tolist(),\n",
    "    \"params\": {\n",
    "        \"langs_keep\": LANGS_KEEP,\n",
    "        \"keep_retweets\": KEEP_RETWEETS,\n",
    "        \"keep_quotes\": KEEP_QUOTES,\n",
    "        \"keep_replies\": KEEP_REPLIES,\n",
    "        \"min_char_len\": MIN_CHAR_LEN,\n",
    "        \"drop_url_heavy\": DROP_URL_HEAVY,\n",
    "        \"url_heavy_threshold\": URL_HEAVY_THRESHOLD,\n",
    "        \"date_from\": DATE_FROM,\n",
    "        \"date_to\": DATE_TO,\n",
    "    },\n",
    "    \"removals\": removals,\n",
    "    \"created_at_utc\": datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"time_range\": {\n",
    "        \"min_created_at\": (df[\"created_at\"].min().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                           if \"created_at\" in df.columns and df[\"created_at\"].notna().any() else None),\n",
    "        \"max_created_at\": (df[\"created_at\"].max().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                           if \"created_at\" in df.columns and df[\"created_at\"].notna().any() else None),\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(manifest_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "print(f\"Saved processed snapshot:\\n- {proc_path}\\n- sample CSV: {csv_sample_path}\\nManifest: {manifest_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
