{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddc08086-1301-45e3-aeb5-f04dd6bc4b10",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#  Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e953a8-86fa-4928-b523-6dace6d588ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q selenium webdriver-manager langdetect pandas pyarrow tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2ae7dc-1381-49c8-829b-b75a29ff6367",
   "metadata": {},
   "source": [
    "# 1) Project paths and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd852fa-fd97-42f8-8aa9-26bde43a15db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import os, re, json, time, hashlib, unicodedata\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Reproducibility \n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Paths\n",
    "PROJ = Path.cwd().resolve().parents[0] if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "DATA = PROJ / \"data\"\n",
    "RAW = DATA / \"raw\"\n",
    "PROC = DATA / \"processed\"\n",
    "MODELS = PROJ / \"models\"\n",
    "REPORTS = PROJ / \"reports\"\n",
    "FIGS = REPORTS / \"figures\"\n",
    "for p in [RAW, PROC, MODELS, REPORTS, FIGS]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def timestamp():\n",
    "    return datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def latest_file(folder: Path, pattern=\"*.parquet\"):\n",
    "    files = sorted(folder.glob(pattern), key=lambda p: p.stat().st_mtime)\n",
    "    return files[-1] if files else None\n",
    "\n",
    "RUN_ID = timestamp()\n",
    "RUN_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a80cd4-7f21-46f2-818d-c6ad1c55f206",
   "metadata": {},
   "source": [
    "# 2) Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b6d96b-6d84-4c74-9aa1-e86d833d1223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "def hash_username(username: str) -> str:\n",
    "    \"\"\"Create a consistent hash for a username.\"\"\"\n",
    "    return hashlib.sha256(username.encode()).hexdigest()[:16]\n",
    "\n",
    "def detect_language(text: str) -> str:\n",
    "    \"\"\"Detect the language of the text, defaulting to 'en' if uncertain.\"\"\"\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return 'en'\n",
    "\n",
    "def parse_date(date_str: str) -> datetime:\n",
    "    \"\"\"Parse Nitter date format into datetime object.\"\"\"\n",
    "    now = datetime.now()\n",
    "    try:\n",
    "        if not date_str:\n",
    "            return now\n",
    "\n",
    "        # Handle the title attribute format (e.g. \"Mar 23, 2025 · 5:15 PM UTC\")\n",
    "        if '·' in date_str:\n",
    "            parts = date_str.split('·')\n",
    "            if len(parts) >= 2:\n",
    "                date_part = parts[0].strip()\n",
    "                time_part = parts[1].strip().replace(' UTC', '')\n",
    "                full_datetime_str = f\"{date_part} {time_part}\"\n",
    "                return datetime.strptime(full_datetime_str, '%b %d, %Y %I:%M %p')\n",
    "\n",
    "        if 'ago' in date_str:\n",
    "            # Relative times\n",
    "            if 'h ago' in date_str:\n",
    "                hours = int(date_str.split('h')[0])\n",
    "                return now - timedelta(hours=hours)\n",
    "            elif 'm ago' in date_str:\n",
    "                minutes = int(date_str.split('m')[0])\n",
    "                return now - timedelta(minutes=minutes)\n",
    "            elif 'd ago' in date_str:\n",
    "                days = int(date_str.split('d')[0])\n",
    "                return now - timedelta(days=days)\n",
    "        else:\n",
    "            # Absolute dates\n",
    "            if ',' in date_str:  # \"Dec 25, 2023\"\n",
    "                return datetime.strptime(date_str, '%b %d, %Y')\n",
    "            else:  # \"Dec 25\"\n",
    "                date = datetime.strptime(date_str, '%b %d')\n",
    "                result = date.replace(year=now.year)\n",
    "                if result > now:\n",
    "                    result = result.replace(year=now.year - 1)\n",
    "                return result\n",
    "    except:\n",
    "        return now  # Fallback to current time\n",
    "\n",
    "def extract_tweet_id(url: str):\n",
    "    \"\"\"Extract tweet ID from the Nitter URL.\"\"\"\n",
    "    try:\n",
    "        match = re.search(r'/status/(\\d+)', url or '')\n",
    "        return match.group(1) if match else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_urls(tweet_element) -> List[str]:\n",
    "    \"\"\"Extract URLs from tweet.\"\"\"\n",
    "    try:\n",
    "        urls = []\n",
    "        link_elements = tweet_element.find_elements(By.CSS_SELECTOR, '.tweet-content a')\n",
    "        for link in link_elements:\n",
    "            url = link.get_attribute('href')\n",
    "            if url and not url.startswith('/'):  # Exclude internal Nitter links\n",
    "                urls.append(url)\n",
    "        return urls\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def is_retweet(container) -> bool:\n",
    "    \"\"\"Check if the tweet is a retweet.\"\"\"\n",
    "    try:\n",
    "        retweet_header = container.find_elements(By.CSS_SELECTOR, '.retweet-header')\n",
    "        return len(retweet_header) > 0\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def is_quote_tweet(container) -> bool:\n",
    "    \"\"\"Check if the tweet is a quote tweet.\"\"\"\n",
    "    try:\n",
    "        quote_container = container.find_elements(By.CSS_SELECTOR, '.quote')\n",
    "        return len(quote_container) > 0\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def extract_engagement_stats(container) -> Dict[str, int]:\n",
    "    \"\"\"Extract retweet, like, and comment counts using CSS selectors.\"\"\"\n",
    "    try:\n",
    "        stats = {'retweet_count': 0, 'like_count': 0, 'comment_count': 0}\n",
    "        \n",
    "        def parse_count(element):\n",
    "            try:\n",
    "                if element:\n",
    "                    text = element.text or \"\"\n",
    "                    text = text.strip().lower()\n",
    "                    if not text:\n",
    "                        return 0\n",
    "                    # Convert abbreviated numbers (e.g., \"1.2k\")\n",
    "                    if 'k' in text:\n",
    "                        return int(float(text.replace('k', '')) * 1000)\n",
    "                    if 'm' in text:\n",
    "                        return int(float(text.replace('m', '')) * 1_000_000)\n",
    "                    # Remove non-digits and convert\n",
    "                    digits = ''.join(filter(str.isdigit, text))\n",
    "                    return int(digits) if digits else 0\n",
    "                return 0\n",
    "            except:\n",
    "                return 0\n",
    "        \n",
    "        try:\n",
    "            comment_element = container.find_element(By.CSS_SELECTOR, '.icon-comment').find_element(By.XPATH, '..')\n",
    "            stats['comment_count'] = parse_count(comment_element)\n",
    "        except Exception:\n",
    "            pass\n",
    "            \n",
    "        try:\n",
    "            retweet_element = container.find_element(By.CSS_SELECTOR, '.icon-retweet').find_element(By.XPATH, '..')\n",
    "            stats['retweet_count'] = parse_count(retweet_element)\n",
    "        except Exception:\n",
    "            pass\n",
    "            \n",
    "        try:\n",
    "            like_element = container.find_element(By.CSS_SELECTOR, '.icon-heart').find_element(By.XPATH, '..')\n",
    "            stats['like_count'] = parse_count(like_element)\n",
    "        except Exception:\n",
    "            pass\n",
    "                \n",
    "        return stats\n",
    "    except Exception:\n",
    "        return {'retweet_count': 0, 'like_count': 0, 'comment_count': 0}\n",
    "\n",
    "def _normalize_host(nitter_host: str) -> str:\n",
    "    nh = (nitter_host or \"https://nitter.net\").strip().rstrip('/')\n",
    "    if not nh.startswith(\"http\"):\n",
    "        nh = \"https://\" + nh\n",
    "    return nh\n",
    "\n",
    "def scrape_x_posts(\n",
    "    username: str,\n",
    "    num_scrolls: int = 5,\n",
    "    tweet_type: str = 'original',\n",
    "    nitter_host: str = \"https://nitter.net\",\n",
    "    headless: bool = True,\n",
    "    user_agent: str = None,\n",
    "    initial_wait_s: float = 5.0\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Scrapes posts from Nitter with enhanced data collection.\n",
    "    \n",
    "    Args:\n",
    "        username (str): The Twitter username to scrape\n",
    "        num_scrolls (int): Number of times to scroll/load more tweets\n",
    "        tweet_type (str): 'original' | 'original_and_quotes' | 'all'\n",
    "        nitter_host (str): Base Nitter host, e.g., \"https://nitter.net\"\n",
    "        headless (bool): Run Chrome in headless mode\n",
    "        user_agent (str): Optional custom user-agent\n",
    "        initial_wait_s (float): Initial wait to let page settle\n",
    "    \"\"\"\n",
    "    nitter_host = _normalize_host(nitter_host)\n",
    "    \n",
    "    os.environ[\"WDM_LOG\"] = \"0\"  # silence webdriver-manager\n",
    "    options = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        # modern headless\n",
    "        options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(\"--lang=en-US,en\")\n",
    "    if user_agent:\n",
    "        options.add_argument(f\"--user-agent={user_agent}\")\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    wait = WebDriverWait(driver, 12)\n",
    "    \n",
    "    try:\n",
    "        profile_url = f\"{nitter_host}/{username}\"\n",
    "        driver.get(profile_url)\n",
    "        time.sleep(initial_wait_s)  # Initial load\n",
    "        \n",
    "        posts = []\n",
    "        scroll_count = 0\n",
    "        user_id_hashed = hash_username(username)\n",
    "        seen_ids = set()  # Track seen tweet IDs\n",
    "        \n",
    "        while scroll_count < num_scrolls:\n",
    "            try:\n",
    "                tweet_containers = wait.until(\n",
    "                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, '.timeline-item'))\n",
    "                )\n",
    "\n",
    "                for container in tweet_containers:\n",
    "                    try:\n",
    "                        # Filter by tweet type\n",
    "                        is_rt = is_retweet(container)\n",
    "                        is_quote = is_quote_tweet(container)\n",
    "                        if tweet_type == 'original' and (is_rt or is_quote):\n",
    "                            continue\n",
    "                        elif tweet_type == 'original_and_quotes' and is_rt:\n",
    "                            continue\n",
    "                        # 'all' includes everything\n",
    "                            \n",
    "                        # Identify tweet URL and ID\n",
    "                        date_element = container.find_element(By.CSS_SELECTOR, '.tweet-date a')\n",
    "                        tweet_url = date_element.get_attribute('href')\n",
    "                        tweet_id = extract_tweet_id(tweet_url)\n",
    "                        if not tweet_id or tweet_id in seen_ids:\n",
    "                            continue\n",
    "                        seen_ids.add(tweet_id)\n",
    "                            \n",
    "                        # Content\n",
    "                        tweet_element = container.find_element(By.CSS_SELECTOR, '.tweet-content')\n",
    "                        tweet_text = tweet_element.text.strip()\n",
    "                        \n",
    "                        # Prefer the title attribute for precise datetime\n",
    "                        date_text = date_element.get_attribute('title') or date_element.text.strip()\n",
    "                        parsed_date = parse_date(date_text)\n",
    "                        \n",
    "                        # Engagement and metadata\n",
    "                        stats = extract_engagement_stats(container)\n",
    "                        is_reply = bool(container.find_elements(By.CSS_SELECTOR, '.replying-to'))\n",
    "                        urls = extract_urls(tweet_element)\n",
    "                        \n",
    "                        tweet_data = {\n",
    "                            'tweet_id': tweet_id,\n",
    "                            'username': username,\n",
    "                            'tweet_url': tweet_url,\n",
    "                            'text': tweet_text,\n",
    "                            'created_at': parsed_date.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                            'lang': detect_language(tweet_text),\n",
    "                            'user_id_hashed': user_id_hashed,\n",
    "                            'retweet_count': stats['retweet_count'],\n",
    "                            'like_count': stats['like_count'],\n",
    "                            'comment_count': stats['comment_count'],\n",
    "                            'is_reply': is_reply,\n",
    "                            'is_retweet': is_rt,\n",
    "                            'is_quote': is_quote,\n",
    "                            'urls': urls\n",
    "                        }\n",
    "                        posts.append(tweet_data)\n",
    "                            \n",
    "                    except StaleElementReferenceException:\n",
    "                        continue\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                \n",
    "                # Pagination\n",
    "                try:\n",
    "                    load_more = driver.find_element(By.XPATH, \"//a[contains(text(), 'Load more')]\")\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView();\", load_more)\n",
    "                    time.sleep(0.8)\n",
    "                    load_more.click()\n",
    "                    time.sleep(2.5)\n",
    "                except Exception:\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    time.sleep(2.5)\n",
    "                finally:\n",
    "                    scroll_count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error while scrolling: {str(e)}\")\n",
    "                scroll_count += 1\n",
    "                continue\n",
    "\n",
    "        # Sort posts by date descending\n",
    "        posts.sort(key=lambda x: datetime.strptime(x['created_at'], '%Y-%m-%d %H:%M:%S'), reverse=True)\n",
    "        return posts\n",
    "\n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aec79b-4203-4a1f-91e6-e27b7c970c7b",
   "metadata": {},
   "source": [
    "# 3) Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eebc5a-4af6-4113-831e-6223704b87a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Preferred Nitter host\n",
    "NITTER_HOST = \"https://nitter.net\"  # \"https://nitter.lacontrevoi.fr\"\n",
    "\n",
    "# What to collect: 'original' | 'original_and_quotes' | 'all'\n",
    "TWEET_TYPE = \"original\"\n",
    "\n",
    "# Scrolling/pagination (per user)\n",
    "NUM_SCROLLS = 100\n",
    "\n",
    "# Browser settings\n",
    "HEADLESS = True\n",
    "USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "INITIAL_WAIT_S = 5.0\n",
    "\n",
    "# Users list: read from sources/usernames.txt if present, else fallback\n",
    "usernames_file = PROJ / \"sources\" / \"usernames.txt\"\n",
    "if usernames_file.exists():\n",
    "    USERNAMES = [ln.strip().lstrip(\"@\") for ln in usernames_file.read_text(encoding=\"utf-8\").splitlines() if ln.strip()]\n",
    "else:\n",
    "    USERNAMES = [\"ilyasut\"] \n",
    "\n",
    "USERNAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9f83f4-c834-4be2-8c0f-6042ebf5a490",
   "metadata": {},
   "source": [
    "# 4) Run scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de05070f-7f5e-431d-8da0-3179bab6020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "all_posts = []\n",
    "errors = []\n",
    "\n",
    "start_ts = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "for u in tqdm(USERNAMES, desc=\"Scraping users\"):\n",
    "    try:\n",
    "        posts = scrape_x_posts(\n",
    "            username=u,\n",
    "            num_scrolls=NUM_SCROLLS,\n",
    "            tweet_type=TWEET_TYPE,\n",
    "            nitter_host=NITTER_HOST,\n",
    "            headless=HEADLESS,\n",
    "            user_agent=USER_AGENT,\n",
    "            initial_wait_s=INITIAL_WAIT_S\n",
    "        )\n",
    "        # Tag source host and normalize type\n",
    "        for p in posts:\n",
    "            p[\"nitter_host\"] = NITTER_HOST\n",
    "            p[\"type\"] = \"retweet\" if p.get(\"is_retweet\") else (\"quote\" if p.get(\"is_quote\") else \"original\")\n",
    "        all_posts.extend(posts)\n",
    "        print(f\"Collected {len(posts):,} posts from @{u}\")\n",
    "    except Exception as e:\n",
    "        errors.append({\"username\": u, \"error\": str(e)})\n",
    "        print(f\"Error with @{u}: {e}\")\n",
    "\n",
    "len(all_posts), len(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2903eb2f-de0d-442a-b1ac-f80d04bd01fa",
   "metadata": {},
   "source": [
    "# 5) Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46290eb-ad3c-4de2-b499-72f5d9c1078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not all_posts:\n",
    "    raise RuntimeError(\"No posts collected. Try reducing NUM_SCROLLS, switching NITTER_HOST, or disabling headless to debug.\")\n",
    "\n",
    "df = pd.DataFrame(all_posts)\n",
    "\n",
    "# Types\n",
    "if \"created_at\" in df.columns:\n",
    "    df[\"created_at\"] = pd.to_datetime(df[\"created_at\"], errors=\"coerce\")\n",
    "\n",
    "# Dedupe on tweet_id (global uniqueness); fall back to username + text if needed\n",
    "before = len(df)\n",
    "if \"tweet_id\" in df.columns:\n",
    "    df = df.drop_duplicates(subset=[\"tweet_id\"])\n",
    "else:\n",
    "    df = df.drop_duplicates(subset=[\"username\", \"text\", \"created_at\"])\n",
    "after = len(df)\n",
    "removed = before - after\n",
    "\n",
    "# Sort newest first\n",
    "if \"created_at\" in df.columns:\n",
    "    df = df.sort_values(\"created_at\", ascending=False)\n",
    "\n",
    "print(f\"Rows before dedupe: {before:,}, after: {after:,} (removed {removed:,})\")\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e815e9-38eb-4fbd-92e9-edd1d0cc64aa",
   "metadata": {},
   "source": [
    "# 6) Save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1683d97-3975-473a-8e52-fd7bfb66defb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = RUN_ID\n",
    "raw_path = RAW / f\"x_posts_{ts}.parquet\"\n",
    "csv_path = RAW / f\"x_posts_{ts}.csv\"  # optional CSV for quick look\n",
    "manifest_path = RAW / f\"manifest_{ts}.json\"\n",
    "\n",
    "# Save Parquet (pyarrow handles list-typed 'urls')\n",
    "df.to_parquet(raw_path, index=False)\n",
    "\n",
    "# Also CSV (serialize lists to JSON strings)\n",
    "df_csv = df.copy()\n",
    "if \"urls\" in df_csv.columns:\n",
    "    df_csv[\"urls\"] = df_csv[\"urls\"].apply(lambda v: json.dumps(v) if isinstance(v, (list, tuple)) else v)\n",
    "df_csv.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "manifest = {\n",
    "    \"run_id\": ts,\n",
    "    \"paths\": {\n",
    "        \"parquet\": str(raw_path.relative_to(PROJ)),\n",
    "        \"csv\": str(csv_path.relative_to(PROJ)),\n",
    "    },\n",
    "    \"rows\": int(len(df)),\n",
    "    \"columns\": df.columns.tolist(),\n",
    "    \"users\": USERNAMES,\n",
    "    \"users_count\": len(USERNAMES),\n",
    "    \"params\": {\n",
    "        \"nitter_host\": NITTER_HOST,\n",
    "        \"tweet_type\": TWEET_TYPE,\n",
    "        \"num_scrolls\": NUM_SCROLLS,\n",
    "        \"headless\": HEADLESS,\n",
    "        \"user_agent\": USER_AGENT,\n",
    "    },\n",
    "    \"collected_at_utc\": datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"min_created_at\": (df[\"created_at\"].min().strftime(\"%Y-%m-%d %H:%M:%S\") if \"created_at\" in df else None),\n",
    "    \"max_created_at\": (df[\"created_at\"].max().strftime(\"%Y-%m-%d %H:%M:%S\") if \"created_at\" in df else None),\n",
    "    \"deduplicated\": {\n",
    "        \"removed\": removed,\n",
    "        \"key\": \"tweet_id\" if \"tweet_id\" in df.columns else \"username,text,created_at\"\n",
    "    },\n",
    "    \"errors\": errors,\n",
    "}\n",
    "\n",
    "with open(manifest_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "print(f\"Saved raw snapshot to:\\n- {raw_path}\\n- {csv_path}\\nManifest: {manifest_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2788974-24da-4629-9643-8c5e9f29f41b",
   "metadata": {},
   "source": [
    "# 7) Quick checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f90278-4572-48e7-a528-77025c840e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"By language (top 10):\")\n",
    "display(df[\"lang\"].value_counts().head(10))\n",
    "\n",
    "print(\"\\nBy type:\")\n",
    "display(df[\"type\"].value_counts())\n",
    "\n",
    "if \"like_count\" in df.columns and \"retweet_count\" in df.columns:\n",
    "    print(\"\\nBasic engagement stats:\")\n",
    "    display(df[[\"like_count\", \"retweet_count\", \"comment_count\"]].describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
