{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff415dad-c835-4d20-afcc-ebbbc7ee2c89",
   "metadata": {},
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4cabd0-821d-4b4a-a331-481147cd5567",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q pandas pyarrow numpy scikit-learn matplotlib seaborn nltk joblib tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0d1248-4f20-4b4d-8e3d-d7ffdd84e700",
   "metadata": {},
   "source": [
    "# 1) Setup: paths and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd931c7-2747-4f91-a3a6-6967f9c82207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json, math, warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "# Paths (works whether you run from notebooks/ or project root)\n",
    "PROJ = Path.cwd().resolve().parents[0] if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "DATA = PROJ / \"data\"\n",
    "RAW = DATA / \"raw\"\n",
    "PROC = DATA / \"processed\"\n",
    "MODELS = PROJ / \"models\"\n",
    "REPORTS = PROJ / \"reports\"\n",
    "FIGS = REPORTS / \"figures\"\n",
    "for p in [RAW, PROC, MODELS, REPORTS, FIGS]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def timestamp():\n",
    "    return datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def latest_file(folder: Path, pattern=\"*.parquet\"):\n",
    "    files = sorted(folder.glob(pattern), key=lambda p: p.stat().st_mtime)\n",
    "    return files[-1] if files else None\n",
    "\n",
    "RUN_ID = timestamp()\n",
    "RUN_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd12261d-de23-4f0c-b956-1fde0b72e089",
   "metadata": {},
   "source": [
    "# 2) Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96914076-aa8a-4607-9fcd-7c6a17a1514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode: \"auto\" | \"supervised\" | \"unsupervised\"\n",
    "MODE = \"auto\"\n",
    "\n",
    "# Column names\n",
    "TEXT_COL_CANDIDATES = [\"text_clean\", \"text\"]\n",
    "LABEL_COLUMN = \"label\"  # if present, used in supervised mode\n",
    "\n",
    "# Supervised params\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "NGRAM_RANGE = (1, 2)\n",
    "MAX_FEATURES = 50000\n",
    "USE_CLASS_WEIGHT_BALANCED = True\n",
    "\n",
    "# Unsupervised (VADER) params\n",
    "VADER_ONLY_ENGLISH = True        # keep only lang == \"en\" before VADER\n",
    "VADER_POS_THRESHOLD = 0.05       # >= => \"pos\"\n",
    "VADER_NEG_THRESHOLD = -0.05      # <= => \"neg\"\n",
    "\n",
    "# Optional transformer inference (set True to run)\n",
    "USE_TRANSFORMER = False\n",
    "TRANSFORMER_MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"  # English\n",
    "# For multilingual, consider: \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "BATCH_SIZE = 32  # for transformer inference\n",
    "\n",
    "# Artifacts naming base\n",
    "ARTI_BASE = f\"sentiment_{RUN_ID}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab98a2be-655d-44fc-9289-4e8698d45007",
   "metadata": {},
   "source": [
    "# 3) Load processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626c702c-bc43-4e0f-9b96-40f75e20a07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_path = latest_file(PROC, \"*.parquet\")\n",
    "if not proc_path:\n",
    "    raise FileNotFoundError(\"No processed parquet found in data/processed. Run 02_data_cleaning first.\")\n",
    "\n",
    "print(f\"Loading processed snapshot: {proc_path}\")\n",
    "df = pd.read_parquet(proc_path)\n",
    "\n",
    "# Pick text column\n",
    "TEXT_COL = None\n",
    "for c in TEXT_COL_CANDIDATES:\n",
    "    if c in df.columns:\n",
    "        TEXT_COL = c\n",
    "        break\n",
    "if TEXT_COL is None:\n",
    "    raise ValueError(f\"None of the expected text columns found: {TEXT_COL_CANDIDATES}\")\n",
    "\n",
    "# Parse time if available\n",
    "if \"created_at\" in df.columns:\n",
    "    df[\"created_at\"] = pd.to_datetime(df[\"created_at\"], errors=\"coerce\")\n",
    "\n",
    "print(\"Rows:\", len(df), \"| Text column:\", TEXT_COL)\n",
    "if LABEL_COLUMN in df.columns:\n",
    "    print(\"Label column found:\", LABEL_COLUMN, \"| unique:\", df[LABEL_COLUMN].dropna().unique()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd36172-bf15-4655-9857-c458751b4eb1",
   "metadata": {},
   "source": [
    "# 4) Helpers for labels and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76557bd0-7d53-471c-a7ff-756ff69179f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_label(x):\n",
    "    \"\"\"\n",
    "    Map incoming labels to {'neg','neu','pos'} if possible.\n",
    "    Rules:\n",
    "      - numeric > 0 => pos; < 0 => neg; == 0 => neu\n",
    "      - string aliases handled (case-insensitive)\n",
    "    \"\"\"\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    # numeric\n",
    "    try:\n",
    "        xv = float(x)\n",
    "        if xv > 0: return \"pos\"\n",
    "        if xv < 0: return \"neg\"\n",
    "        return \"neu\"\n",
    "    except:\n",
    "        pass\n",
    "    s = str(x).strip().lower()\n",
    "    pos_alias = {\"pos\",\"positive\",\"+\",\"p\",\"good\",\"favorable\",\"favourable\",\"yes\",\"true\",\"1\",\"ðŸ‘\",\"ðŸ™‚\",\"ðŸ˜Š\"}\n",
    "    neg_alias = {\"neg\",\"negative\",\"-\",\"n\",\"bad\",\"unfavorable\",\"unfavourable\",\"no\",\"false\",\"-1\",\"ðŸ‘Ž\",\"ðŸ™\",\"ðŸ˜ž\"}\n",
    "    neu_alias = {\"neu\",\"neutral\",\"0\",\"neut\",\"meh\",\"ok\",\"mixed\"}\n",
    "    if s in pos_alias: return \"pos\"\n",
    "    if s in neg_alias: return \"neg\"\n",
    "    if s in neu_alias: return \"neu\"\n",
    "    # fallback: unknown -> NaN\n",
    "    return np.nan\n",
    "\n",
    "def plot_confusion(cm, labels, title, path):\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=150)\n",
    "    plt.show()\n",
    "    print(\"Saved:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566398eb-dcf8-4f6a-85b5-4a38ecceaf5f",
   "metadata": {},
   "source": [
    "# 5) Decide mode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9d81ce-e6a5-451f-b5e2-ca625392849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resolved_mode = MODE\n",
    "if MODE == \"auto\":\n",
    "    if LABEL_COLUMN in df.columns:\n",
    "        y_raw = df[LABEL_COLUMN].copy()\n",
    "        y_std = y_raw.map(standardize_label)\n",
    "        if y_std.notna().sum() >= 50 and y_std.dropna().nunique() >= 2:\n",
    "            resolved_mode = \"supervised\"\n",
    "        else:\n",
    "            resolved_mode = \"unsupervised\"\n",
    "    else:\n",
    "        resolved_mode = \"unsupervised\"\n",
    "\n",
    "print(\"Resolved MODE:\", resolved_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f4e0df-a906-4413-b364-44e29e61cc69",
   "metadata": {},
   "source": [
    "# 6) Supervised baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9954d11f-2df1-433a-b1db-46f77fd29782",
   "metadata": {},
   "outputs": [],
   "source": [
    "if resolved_mode == \"supervised\":\n",
    "    # Standardize and filter labels\n",
    "    y = df[LABEL_COLUMN].map(standardize_label)\n",
    "    mask = y.notna()\n",
    "    X_text = df.loc[mask, TEXT_COL].astype(str)\n",
    "    y = y.loc[mask]\n",
    "    print(f\"Training rows after label filtering: {len(y):,} | classes: {sorted(y.unique())}\")\n",
    "\n",
    "    # Train/test split\n",
    "    stratify = y if y.nunique() > 1 else None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_text, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=stratify\n",
    "    )\n",
    "\n",
    "    # Pipeline\n",
    "    tfidf = TfidfVectorizer(lowercase=True, stop_words=\"english\",\n",
    "                            ngram_range=NGRAM_RANGE, max_features=MAX_FEATURES)\n",
    "    clf = LogisticRegression(max_iter=1000, class_weight=(\"balanced\" if USE_CLASS_WEIGHT_BALANCED else None), solver=\"lbfgs\", multi_class=\"auto\")\n",
    "    pipe = Pipeline([(\"tfidf\", tfidf), (\"clf\", clf)])\n",
    "\n",
    "    # Fit\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    overall = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"macro_f1\": f1_score(y_test, y_pred, average=\"macro\"),\n",
    "        \"weighted_f1\": f1_score(y_test, y_pred, average=\"weighted\"),\n",
    "    }\n",
    "    print(\"Overall:\", overall)\n",
    "    print(pd.DataFrame(report).T)\n",
    "\n",
    "    # Confusion matrix\n",
    "    labels = sorted(y.unique())\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "    fig_cm = FIGS / f\"{ARTI_BASE}_confusion_matrix.png\"\n",
    "    plot_confusion(cm, labels, \"Confusion matrix (TF-IDF + LogReg)\", fig_cm)\n",
    "\n",
    "    # Probabilities and prediction export\n",
    "    proba = None\n",
    "    try:\n",
    "        proba = pipe.predict_proba(X_test)\n",
    "        max_p = proba.max(axis=1)\n",
    "    except Exception:\n",
    "        max_p = np.ones(len(y_pred))\n",
    "\n",
    "    pred_df = pd.DataFrame({\n",
    "        \"text\": X_test.values,\n",
    "        \"y_true\": y_test.values,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"confidence\": max_p\n",
    "    })\n",
    "    # Attach ids and timestamps if present\n",
    "    for col in [\"tweet_id\",\"username\",\"created_at\"]:\n",
    "        if col in df.columns:\n",
    "            pred_df[col] = df.loc[X_test.index, col].values\n",
    "\n",
    "    preds_path = REPORTS / f\"{ARTI_BASE}_supervised_predictions.csv\"\n",
    "    pred_df.to_csv(preds_path, index=False, encoding=\"utf-8\")\n",
    "    print(\"Saved predictions:\", preds_path)\n",
    "\n",
    "    # Save model\n",
    "    model_path = MODELS / f\"{ARTI_BASE}_tfidf_logreg.joblib\"\n",
    "    joblib.dump(pipe, model_path)\n",
    "    print(\"Saved model:\", model_path)\n",
    "\n",
    "    # Save metrics\n",
    "    metrics = {\n",
    "        \"run_id\": RUN_ID,\n",
    "        \"mode\": \"supervised\",\n",
    "        \"input\": str(proc_path.relative_to(PROJ)),\n",
    "        \"n_train\": int(len(X_train)),\n",
    "        \"n_test\": int(len(X_test)),\n",
    "        \"labels\": labels,\n",
    "        \"overall\": overall,\n",
    "        \"classification_report\": report,\n",
    "        \"params\": {\n",
    "            \"test_size\": TEST_SIZE,\n",
    "            \"random_state\": RANDOM_STATE,\n",
    "            \"ngram_range\": NGRAM_RANGE,\n",
    "            \"max_features\": MAX_FEATURES,\n",
    "            \"class_weight\": \"balanced\" if USE_CLASS_WEIGHT_BALANCED else None\n",
    "        }\n",
    "    }\n",
    "    metrics_path = REPORTS / f\"{ARTI_BASE}_supervised_metrics.json\"\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(\"Saved metrics:\", metrics_path)\n",
    "\n",
    "    # Feature importance (per class)\n",
    "    try:\n",
    "        vec = pipe.named_steps[\"tfidf\"]\n",
    "        clf = pipe.named_steps[\"clf\"]\n",
    "        vocab = np.array(vec.get_feature_names_out())\n",
    "\n",
    "        coef = clf.coef_\n",
    "        classes = clf.classes_\n",
    "        top_k = 30\n",
    "\n",
    "        for i, cls in enumerate(classes):\n",
    "            # top positive weights for class cls\n",
    "            order = np.argsort(coef[i])[::-1][:top_k]\n",
    "            terms = pd.DataFrame({\"term\": vocab[order], \"weight\": coef[i][order]})\n",
    "            out = REPORTS / f\"{ARTI_BASE}_top_terms_class_{cls}.csv\"\n",
    "            terms.to_csv(out, index=False, encoding=\"utf-8\")\n",
    "            print(f\"Saved top terms for class {cls} -> {out}\")\n",
    "    except Exception as e:\n",
    "        print(\"Skipping feature importance:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e891c-e2f1-464f-8260-1e09c380b8bf",
   "metadata": {},
   "source": [
    "# 7) Unsupervised baseline (VADER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94544833-7ead-44cf-9d6e-026850ce5365",
   "metadata": {},
   "outputs": [],
   "source": [
    "if resolved_mode == \"unsupervised\":\n",
    "    # Optionally filter to English for VADER\n",
    "    df_v = df.copy()\n",
    "    if VADER_ONLY_ENGLISH and \"lang\" in df_v.columns:\n",
    "        before = len(df_v)\n",
    "        df_v = df_v[df_v[\"lang\"] == \"en\"]\n",
    "        print(f\"VADER: kept English only: {len(df_v):,}/{before:,}\")\n",
    "\n",
    "    # Prepare text\n",
    "    texts = df_v[TEXT_COL].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "    # VADER scoring\n",
    "    try:\n",
    "        import nltk\n",
    "        nltk.download(\"vader_lexicon\", quiet=True)\n",
    "        from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "        sia = SentimentIntensityAnalyzer()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"Install nltk to use VADER: pip install nltk\") from e\n",
    "\n",
    "    def comp_to_label(c):\n",
    "        if c >= VADER_POS_THRESHOLD: return \"pos\"\n",
    "        if c <= VADER_NEG_THRESHOLD: return \"neg\"\n",
    "        return \"neu\"\n",
    "\n",
    "    compounds = [sia.polarity_scores(t)[\"compound\"] for t in texts]\n",
    "    vader_labels = [comp_to_label(c) for c in compounds]\n",
    "\n",
    "    # Build output frame\n",
    "    out_df = df_v.copy()\n",
    "    out_df[\"vader_compound\"] = compounds\n",
    "    out_df[\"vader_label\"] = vader_labels\n",
    "\n",
    "    # If ground-truth labels exist, evaluate\n",
    "    eval_report = {}\n",
    "    if LABEL_COLUMN in out_df.columns:\n",
    "        gt = out_df[LABEL_COLUMN].map(standardize_label)\n",
    "        mask = gt.notna()\n",
    "        if mask.sum() >= 20 and gt[mask].nunique() >= 2:\n",
    "            y_true = gt[mask]\n",
    "            y_pred = pd.Series(out_df.loc[mask, \"vader_label\"].values, index=y_true.index)\n",
    "            rep = classification_report(y_true, y_pred, output_dict=True)\n",
    "            eval_report = {\n",
    "                \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "                \"macro_f1\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "                \"weighted_f1\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "                \"classification_report\": rep\n",
    "            }\n",
    "            print(\"VADER vs labels (where available):\")\n",
    "            print(pd.DataFrame(rep).T)\n",
    "\n",
    "    # Aggregates\n",
    "    label_counts = out_df[\"vader_label\"].value_counts().to_dict()\n",
    "    avg_compound = float(np.mean(out_df[\"vader_compound\"])) if len(out_df) else None\n",
    "\n",
    "    print(\"Label distribution:\", label_counts)\n",
    "    print(\"Average compound:\", avg_compound)\n",
    "\n",
    "    # Save artifacts\n",
    "    out_parquet = REPORTS / f\"{ARTI_BASE}_vader_scores.parquet\"\n",
    "    out_csv = REPORTS / f\"{ARTI_BASE}_vader_scores_sample.csv\"\n",
    "    out_df.to_parquet(out_parquet, index=False)\n",
    "    out_df.head(2000).to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "    print(\"Saved:\", out_parquet)\n",
    "    print(\"Saved sample CSV:\", out_csv)\n",
    "\n",
    "    # Time trend plot\n",
    "    if \"created_at\" in out_df.columns and out_df[\"created_at\"].notna().any():\n",
    "        daily = (out_df.set_index(\"created_at\")[\"vader_compound\"]\n",
    "                 .resample(\"D\").mean().reset_index())\n",
    "        plt.figure(figsize=(8,3))\n",
    "        sns.lineplot(data=daily, x=\"created_at\", y=\"vader_compound\")\n",
    "        plt.title(\"VADER: average compound per day\")\n",
    "        plt.tight_layout()\n",
    "        fig_path = FIGS / f\"{ARTI_BASE}_vader_daily.png\"\n",
    "        plt.savefig(fig_path, dpi=150)\n",
    "        plt.show()\n",
    "        print(\"Saved:\", fig_path)\n",
    "\n",
    "    # Save run summary\n",
    "    summary = {\n",
    "        \"run_id\": RUN_ID,\n",
    "        \"mode\": \"unsupervised\",\n",
    "        \"input\": str(proc_path.relative_to(PROJ)),\n",
    "        \"rows_scored\": int(len(out_df)),\n",
    "        \"text_col\": TEXT_COL,\n",
    "        \"vader_thresholds\": {\"pos\": VADER_POS_THRESHOLD, \"neg\": VADER_NEG_THRESHOLD},\n",
    "        \"label_distribution\": label_counts,\n",
    "        \"avg_compound\": avg_compound,\n",
    "        \"evaluation_if_labels\": eval_report\n",
    "    }\n",
    "    summary_path = REPORTS / f\"{ARTI_BASE}_vader_summary.json\"\n",
    "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(\"Saved summary:\", summary_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27a9ac8-0df6-4ca4-b7c1-afaf4a9c8347",
   "metadata": {},
   "source": [
    "# 8) Transformer sentiment inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532578e7-6a60-4c6c-a9c1-02c79f9e9fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_TRANSFORMER:\n",
    "    try:\n",
    "        from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "        import torch\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"Install transformers/torch to run this block. See the pip cell above.\") from e\n",
    "\n",
    "    model_name = TRANSFORMER_MODEL\n",
    "    print(\"Loading model:\", model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "    pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer,\n",
    "                                      device=device, truncation=True, top_k=None, return_all_scores=True)\n",
    "\n",
    "    df_t = df.copy()\n",
    "    texts = df_t[TEXT_COL].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "    # Batch inference\n",
    "    preds_all = []\n",
    "    for i in range(0, len(texts), BATCH_SIZE):\n",
    "        batch = texts[i:i+BATCH_SIZE]\n",
    "        preds_all.extend(pipe(batch))\n",
    "\n",
    "    # Convert outputs to scores and labels\n",
    "    # CardiffNLP label order typically: 0=negative, 1=neutral, 2=positive\n",
    "    def parse_scores(entry):\n",
    "        try:\n",
    "            scores = {d[\"label\"].lower(): float(d[\"score\"]) for d in entry}\n",
    "            # Normalize common label names\n",
    "            mapping = {}\n",
    "            for k, v in scores.items():\n",
    "                if \"neg\" in k: mapping[\"neg\"] = v\n",
    "                elif \"neu\" in k: mapping[\"neu\"] = v\n",
    "                elif \"pos\" in k: mapping[\"pos\"] = v\n",
    "            if not mapping:\n",
    "                # Fall back to max label if not matched\n",
    "                best = max(scores.items(), key=lambda x: x[1])[0].lower()\n",
    "                if \"neg\" in best: mapping = {\"neg\": 1.0, \"neu\": 0.0, \"pos\": 0.0}\n",
    "                elif \"neu\" in best: mapping = {\"neg\": 0.0, \"neu\": 1.0, \"pos\": 0.0}\n",
    "                else: mapping = {\"neg\": 0.0, \"neu\": 0.0, \"pos\": 1.0}\n",
    "            # Pred label\n",
    "            pred = max(mapping.items(), key=lambda x: x[1])[0]\n",
    "            return mapping.get(\"neg\", np.nan), mapping.get(\"neu\", np.nan), mapping.get(\"pos\", np.nan), pred\n",
    "        except Exception:\n",
    "            return np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    neg_scores, neu_scores, pos_scores, preds = [], [], [], []\n",
    "    for entry in preds_all:\n",
    "        n, u, p, lbl = parse_scores(entry)\n",
    "        neg_scores.append(n); neu_scores.append(u); pos_scores.append(p); preds.append(lbl)\n",
    "\n",
    "    out_df = df_t.copy()\n",
    "    out_df[\"hf_neg\"] = neg_scores\n",
    "    out_df[\"hf_neu\"] = neu_scores\n",
    "    out_df[\"hf_pos\"] = pos_scores\n",
    "    out_df[\"hf_label\"] = preds\n",
    "\n",
    "    # Evaluate if labels exist\n",
    "    eval_report = {}\n",
    "    if LABEL_COLUMN in out_df.columns:\n",
    "        gt = out_df[LABEL_COLUMN].map(standardize_label)\n",
    "        mask = gt.notna()\n",
    "        if mask.sum() >= 20 and gt[mask].nunique() >= 2:\n",
    "            y_true = gt[mask]\n",
    "            y_pred = out_df.loc[mask, \"hf_label\"]\n",
    "            rep = classification_report(y_true, y_pred, output_dict=True)\n",
    "            eval_report = {\n",
    "                \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "                \"macro_f1\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "                \"weighted_f1\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "                \"classification_report\": rep\n",
    "            }\n",
    "            print(\"Transformer vs labels:\")\n",
    "            print(pd.DataFrame(rep).T)\n",
    "\n",
    "    out_parquet = REPORTS / f\"{ARTI_BASE}_hf_{Path(model_name).name}_scores.parquet\"\n",
    "    out_csv = REPORTS / f\"{ARTI_BASE}_hf_{Path(model_name).name}_scores_sample.csv\"\n",
    "    out_df.to_parquet(out_parquet, index=False)\n",
    "    out_df.head(2000).to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "    print(\"Saved:\", out_parquet)\n",
    "    print(\"Saved sample CSV:\", out_csv)\n",
    "\n",
    "    summary = {\n",
    "        \"run_id\": RUN_ID,\n",
    "        \"mode\": \"transformer_inference\",\n",
    "        \"model\": model_name,\n",
    "        \"input\": str(proc_path.relative_to(PROJ)),\n",
    "        \"rows_scored\": int(len(out_df)),\n",
    "        \"evaluation_if_labels\": eval_report\n",
    "    }\n",
    "    summary_path = REPORTS / f\"{ARTI_BASE}_hf_{Path(model_name).name}_summary.json\"\n",
    "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(\"Saved summary:\", summary_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
